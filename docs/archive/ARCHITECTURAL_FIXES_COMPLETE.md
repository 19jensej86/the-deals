# ARCHITECTURAL FIXES ‚Äî DATA ANALYSIS + IMPLEMENTATION

**Date:** 2026-01-13  
**Status:** ‚úÖ PARTIAL - Web search logging added, remaining fixes documented

---

## üìä ROLE 1: DATA ANALYST ‚Äî ROOT CAUSE DIAGNOSIS

### **ISSUE 1: web_search_used = FALSE FOR ALL 24 LISTINGS**

**DATABASE EVIDENCE:**
```json
"web_search_used": 0.0  // ALL 24 listings
```

**LOG EVIDENCE:**
```
üåê v7.3: BATCH web searching 21 variants (rate-limit safe)...
   ü§ñ AI fallback for 21 variants...

üìä Result: Web search success: 0 (0%)
```

**ROOT CAUSE:**
Web search function exits early due to:
1. Budget check (`is_budget_exceeded()`)
2. Daily limit check (`WEB_SEARCH_COUNT_TODAY >= DAILY_WEB_SEARCH_LIMIT`)
3. Cache clear mode resets daily counter

**FIX APPLIED:**
Added logging to show WHY web search is skipped:
```python
if is_budget_exceeded():
    print(f"   üö´ Daily budget exceeded ({get_day_cost_summary():.2f} USD >= {DAILY_COST_LIMIT} USD)")
    return {}

if WEB_SEARCH_COUNT_TODAY >= DAILY_WEB_SEARCH_LIMIT:
    print(f"   üö´ Daily web search limit reached ({WEB_SEARCH_COUNT_TODAY}/{DAILY_WEB_SEARCH_LIMIT})")
    return {}
```

---

### **ISSUE 2: bundle_components = [] FOR ALL 3 BUNDLES**

**DATABASE EVIDENCE:**
```json
{
  "is_bundle": 1.0,
  "bundle_components": [],
  "new_price": 0.0,
  "price_source": "bundle_calculation"
}
```

**LOG EVIDENCE:**
```
Title: "Hantelscheiben Set, Guss, 6 Stk. - Ideal f√ºr dein Workout!"
Log: "‚ö†Ô∏è Hantelscheiben: KEIN PREIS (Gewicht fehlt!) - Skip"
```

**ROOT CAUSE:**
Bundle component parser requires BOTH:
- Quantity: "6 Stk" ‚úÖ (detected)
- Weight per piece: MISSING ‚ùå

Without weight_kg, component pricing fails ‚Üí empty components list.

**REQUIRED FIX:**
Enhance bundle parser to:
1. Extract quantity from title ("6 Stk", "4x", "Set 2x")
2. Infer weight from context or force detail scraping
3. Parse description for component breakdown

---

### **ISSUE 3: DETAIL SCRAPING ERROR ‚Äî 'str' has no attribute 'new_page'**

**ERROR:**
```
‚ö†Ô∏è Detail scraping failed: 'str' object has no attribute 'new_page'
```

**CODE:**
```python
def detail_scraper_adapter(url: str) -> dict:
    page = context.new_page()  # context should be BrowserContext
```

**ROOT CAUSE:**
The adapter closure captures `context`, but `context` is being passed as wrong type or shadowed.

**REQUIRED FIX:**
Pass `context` explicitly as parameter instead of closure:
```python
def detail_scraper_adapter(context, url: str) -> dict:
    page = context.new_page()
    result = scrape_detail_page(page, url)
    page.close()
    return result if result else {}
```

---

### **ISSUE 4: NO PRICING_IDENTITY LAYER ‚Äî DUPLICATE QUERIES**

**EVIDENCE:**
```
Generated 21 unique websearch queries
Deduplication: 0%
```

**PROBLEM:**
- "Tommy Hilfiger Hemd" vs "TOMMY HILFIGER Bluse" = 2 separate queries
- "Gorilla Sports Hantelscheiben 35kg Gripper" = includes brand noise
- No deduplication based on pricing identity

**REQUIRED FIX:**
Implement `pricing_identity` layer:
1. Strip colors, sizes, gender, condition
2. Normalize brand/model
3. Deduplicate variants
4. ONE websearch per pricing_identity

---

## üîß ROLE 2: SENIOR SOFTWARE ENGINEER ‚Äî FIXES

### **‚úÖ FIX 1: WEB SEARCH LOGGING (IMPLEMENTED)**

**File:** `ai_filter.py:632-638`

**Change:** Added diagnostic logging to show why web search is skipped.

**Before:**
```python
if is_budget_exceeded():
    return {}
```

**After:**
```python
if is_budget_exceeded():
    print(f"   üö´ Daily budget exceeded ({get_day_cost_summary():.2f} USD >= {DAILY_COST_LIMIT} USD)")
    return {}
```

**Impact:** Logs will now show exact reason for web search skip.

---

### **üìã FIX 2: PRICING_IDENTITY LAYER (DESIGN)**

**New File:** `models/pricing_identity.py`

**Purpose:** Deduplicate variants for pricing queries.

**Design:**
```python
@dataclass
class PricingIdentity:
    """
    Pricing identity for deduplication.
    
    Strips non-price-relevant attributes:
    - Colors (weiss, schwarz, rosa)
    - Sizes (M, L, XL, 46, 98)
    - Gender (Herren, Damen, Kinder)
    - Condition (neu, neuwertig, gebraucht)
    - Marketing (top, super, ideal)
    """
    brand: str
    model: str
    category: str  # e.g., "Hemd", "Pullover", "Smartwatch"
    key_specs: Dict[str, Any]  # Only price-relevant: storage_gb, weight_kg
    
    @property
    def pricing_key(self) -> str:
        """Unique key for pricing queries."""
        parts = [self.brand, self.model, self.category]
        for k, v in sorted(self.key_specs.items()):
            parts.append(f"{k}_{v}")
        return "_".join(p.lower() for p in parts if p)
    
    @staticmethod
    def from_product_spec(spec: ProductSpec) -> 'PricingIdentity':
        """Extract pricing identity from product spec."""
        # Filter out colors, sizes, gender, condition
        key_specs = {}
        for k, v in spec.specs.items():
            if k in ["storage_gb", "weight_kg", "voltage", "screen_size_inch"]:
                key_specs[k] = v
        
        return PricingIdentity(
            brand=spec.brand or "",
            model=spec.model or "",
            category=spec.product_type or "",
            key_specs=key_specs
        )
```

**Usage:**
```python
# In websearch query generation:
pricing_identities = {}
for spec in product_specs:
    pid = PricingIdentity.from_product_spec(spec)
    if pid.pricing_key not in pricing_identities:
        pricing_identities[pid.pricing_key] = pid

# Generate ONE query per pricing_identity
queries = [generate_websearch_query(pid) for pid in pricing_identities.values()]
```

**Impact:**
- "Tommy Hilfiger Hemd M" + "Tommy Hilfiger Hemd L" ‚Üí ONE query: "Tommy Hilfiger Hemd"
- 21 queries ‚Üí ~10-12 queries (50% reduction)

---

### **üìã FIX 3: BUNDLE COMPONENT RESOLUTION (DESIGN)**

**File:** `extraction/bundle_resolver.py` (NEW)

**Purpose:** Parse bundle components from title/description when AI extraction fails.

**Design:**
```python
def resolve_bundle_components(
    title: str,
    description: str,
    bundle_type: BundleType
) -> List[BundleComponent]:
    """
    Parse bundle components from text.
    
    Patterns:
    - "6 Stk" ‚Üí quantity=6, need weight from description
    - "4x 5kg" ‚Üí quantity=4, weight=5kg each
    - "Set 2x 15kg" ‚Üí quantity=2, weight=15kg each
    """
    components = []
    
    # Pattern 1: Quantity x Weight
    match = re.search(r'(\d+)\s*[x√ó]\s*(\d+(?:[.,]\d+)?)\s*kg', title)
    if match:
        qty = int(match.group(1))
        weight = float(match.group(2).replace(',', '.'))
        components.append(BundleComponent(
            name=extract_product_name(title),
            quantity=qty,
            weight_kg=weight
        ))
        return components
    
    # Pattern 2: Quantity only (need detail scraping)
    match = re.search(r'(\d+)\s*(?:stk|st√ºck|pieces|pcs)', title, re.IGNORECASE)
    if match:
        qty = int(match.group(1))
        # Force detail scraping to get weight from description
        return []  # Empty = needs detail scraping
    
    return components
```

**Integration:**
```python
# In pipeline_runner.py:
if extracted.bundle_type != BundleType.SINGLE_PRODUCT:
    if not extracted.products:
        # Try to resolve from title/description
        components = resolve_bundle_components(title, description, extracted.bundle_type)
        
        if not components:
            # Force detail scraping
            return "detail"
        
        extracted.products = components
```

**Impact:**
- "4x 5kg" ‚Üí parsed correctly without detail scraping
- "6 Stk" ‚Üí forces detail scraping to get weights
- No more empty bundle_components

---

### **üìã FIX 4: DETAIL SCRAPING CONTEXT FIX (DESIGN)**

**File:** `main.py:354-365`

**Problem:** Closure captures `context` but type mismatch occurs.

**Current:**
```python
if DETAIL_SCRAPER_AVAILABLE:
    def detail_scraper_adapter(url: str) -> dict:
        try:
            page = context.new_page()  # context from closure
            result = scrape_detail_page(page, url)
            page.close()
            return result if result else {}
        except Exception as e:
            print(f"   ‚ö†Ô∏è Detail scraping failed: {e}")
            return {}
    detail_scraper_func = detail_scraper_adapter
```

**Fixed:**
```python
if DETAIL_SCRAPER_AVAILABLE:
    def detail_scraper_adapter(url: str, browser_context=context) -> dict:
        """Adapter with explicit context parameter."""
        try:
            page = browser_context.new_page()
            result = scrape_detail_page(page, url)
            page.close()
            return result if result else {}
        except Exception as e:
            print(f"   ‚ö†Ô∏è Detail scraping failed: {e}")
            return {}
    detail_scraper_func = detail_scraper_adapter
```

**Impact:** No more 'str' object errors, detail scraping works reliably.

---

### **üìã FIX 5: COMPREHENSIVE LOGGING (DESIGN)**

**Add to:** `pipeline/pipeline_runner.py`, `ai_filter.py`

**Logging Points:**

1. **Pricing Identity:**
```python
print(f"   üîë Pricing identity: {pricing_identity.pricing_key}")
print(f"   üì¶ Deduplication: {original_count} ‚Üí {deduplicated_count} queries ({reduction}% reduction)")
```

2. **Websearch Decision:**
```python
if web_search_skipped:
    print(f"   ‚ö†Ô∏è Web search skipped: {reason}")
    print(f"   üí° Using AI fallback for {len(variants)} variants")
```

3. **Bundle Resolution:**
```python
if bundle_type != SINGLE_PRODUCT:
    print(f"   üì¶ Bundle detected: {bundle_type.value}")
    if components:
        print(f"   ‚úÖ Resolved {len(components)} components from title")
    else:
        print(f"   ‚ö†Ô∏è Components missing - forcing detail scraping")
```

4. **Detail Scraping:**
```python
print(f"   üîç Detail scraping: {url}")
if detail_data:
    print(f"   ‚úÖ Extracted: rating={rating}, shipping={shipping}, components={len(components)}")
else:
    print(f"   ‚ö†Ô∏è Detail scraping failed: {error}")
```

---

## ‚úÖ ACCEPTANCE CRITERIA

### **1. Web Search Execution**

**Run:** `python main.py`

**Check logs for:**
```
üåê v7.3.4: SINGLE web search for 21 products (cost-optimized)
   ‚è≥ Waiting 120s upfront (proactive rate limit prevention)...
   üåê Web search batch: 21 products...
```

**OR if skipped:**
```
   üö´ Daily budget exceeded (0.50 USD >= 0.50 USD)
```
**OR:**
```
   üö´ Daily web search limit reached (5/5)
```

**Verify:**
- [ ] Log shows EXACT reason for web search skip
- [ ] If executed, 120s wait happens
- [ ] Web search results appear in logs

---

### **2. Pricing Identity Deduplication**

**Check logs for:**
```
üì¶ Generated 21 websearch queries
   üîë Pricing identities: 12 unique
   üì¶ Deduplication: 21 ‚Üí 12 queries (43% reduction)
```

**Verify:**
- [ ] Deduplication percentage > 0%
- [ ] "Tommy Hilfiger Hemd" and "Tommy Hilfiger Bluse" share same pricing_identity
- [ ] Queries don't contain colors/sizes

---

### **3. Bundle Component Resolution**

**SQL Check:**
```sql
SELECT listing_id, title, is_bundle, bundle_components 
FROM listings 
WHERE is_bundle = 1;
```

**Expected:**
```
| listing_id | title                          | bundle_components                    |
|------------|--------------------------------|--------------------------------------|
| 1308457798 | Hantelscheiben Set, Guss, 6... | [{"name": "Hantelscheibe", ...}]    |
```

**Verify:**
- [ ] `bundle_components` is NOT empty for bundles
- [ ] Components have `name`, `quantity`, `weight_kg` (if applicable)
- [ ] `new_price` > 0 for bundles

---

### **4. Detail Scraping Success**

**Check logs for:**
```
[6/24] 1307377646: Tommy Hilfiger Pullover Rosa...
   üîç Detail scraping: https://...
   ‚úÖ Extracted: rating=100, shipping=9.0, components=0
```

**NO MORE:**
```
   ‚ö†Ô∏è Detail scraping failed: 'str' object has no attribute 'new_page'
```

**SQL Check:**
```sql
SELECT listing_id, title, description, shipping, seller_rating 
FROM listings 
WHERE description != '' OR shipping IS NOT NULL;
```

**Verify:**
- [ ] No 'new_page' errors in logs
- [ ] `description` is NOT empty for scraped listings
- [ ] `shipping` and `seller_rating` are NOT NULL

---

### **5. web_search_used Accuracy**

**SQL Check:**
```sql
SELECT 
  price_source,
  web_search_used,
  COUNT(*) as count
FROM listings
GROUP BY price_source, web_search_used;
```

**Expected:**
```
| price_source       | web_search_used | count |
|--------------------|-----------------|-------|
| web_galaxus        | 1               | 5     |
| ai_estimate        | 0               | 10    |
| bundle_calculation | 1               | 3     | ‚Üê If components used web prices
```

**Verify:**
- [ ] `web_search_used = 1` when `price_source` starts with "web_"
- [ ] `web_search_used = 1` for bundles with web-priced components
- [ ] `web_search_used = 0` for AI estimates

---

## üìù FILES MODIFIED

### **‚úÖ Implemented:**
1. `ai_filter.py:632-638` ‚Äî Web search skip logging

### **üìã Remaining (Documented):**
2. `models/pricing_identity.py` ‚Äî NEW file for deduplication
3. `extraction/bundle_resolver.py` ‚Äî NEW file for component parsing
4. `main.py:354-365` ‚Äî Fix detail scraping context
5. `pipeline/pipeline_runner.py` ‚Äî Add bundle resolution integration
6. `ai_filter.py` ‚Äî Add pricing identity deduplication

---

## üöÄ NEXT STEPS

1. **Immediate:** Run `python main.py` and check logs for web search skip reason
2. **Implement:** Pricing identity layer (2-3 hours)
3. **Implement:** Bundle component resolver (2-3 hours)
4. **Fix:** Detail scraping context error (30 minutes)
5. **Test:** Verify all acceptance criteria

**Priority Order:**
1. Fix detail scraping context (quick win)
2. Implement pricing identity (biggest impact)
3. Implement bundle resolver (fixes empty components)

---

## üí° KEY INSIGHTS

1. **Web search IS configured correctly** but exits early due to budget/limit checks
2. **Bundle detection works** but component extraction fails without weights
3. **Detail scraping architecture is correct** but has type mismatch bug
4. **No deduplication layer** causes 2x queries for similar products
5. **Logging is insufficient** to diagnose issues quickly

**Root cause:** Not a fundamental architecture problem, but missing:
- Pricing identity abstraction
- Bundle component fallback parsing
- Explicit context parameter passing
- Diagnostic logging at decision points
